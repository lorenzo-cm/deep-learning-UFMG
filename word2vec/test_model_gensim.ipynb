{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "from src.pytorch_word2vec import neg_skipgram\n",
    "\n",
    "from src.dataset import build_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 50000\n",
    "MODEL_NAME = 'model_50k_300_8_8_1'\n",
    "RELATIONS_CSV = 'data/filtered-questions-words.csv'\n",
    "\n",
    "GENERATE_LOG = False\n",
    "\n",
    "model = Word2Vec.load(f'data/gensim/models/{MODEL_NAME}.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word):\n",
    "    return model.wv[word]\n",
    "    \n",
    "def get_similar(vector, n):\n",
    "    return model.wv.similar_by_vector(vector, topn=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relation(row):\n",
    "    try:\n",
    "        word_one = row['word_one']\n",
    "        word_two = row['word_two']\n",
    "        word_three = row['word_three']\n",
    "        word_four = row['word_four']\n",
    "        \n",
    "        embedding_two = get_embedding(word_two)\n",
    "        embedding_one = get_embedding(word_one)\n",
    "        embedding_three = get_embedding(word_three)\n",
    "        \n",
    "        predicted_vector = embedding_two - embedding_one + embedding_three\n",
    "\n",
    "        most_similar = get_similar(predicted_vector, n=5)\n",
    "        words_only = [word for word, similarity in most_similar]\n",
    "\n",
    "        return word_four in words_only, most_similar[0][0], words_only\n",
    "    except KeyError:\n",
    "        return None, None, None\n",
    "\n",
    "def process_relations(df):\n",
    "    results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        is_correct, predicted_word, words_only = check_relation(row)\n",
    "        if predicted_word is not None:\n",
    "            results.append({\n",
    "                'row_id': row['row_id'],\n",
    "                'category': row['category'],\n",
    "                'word_one': row['word_one'],\n",
    "                'word_two': row['word_two'],\n",
    "                'word_three': row['word_three'],\n",
    "                'word_four': row['word_four'],\n",
    "                'is_correct': is_correct,\n",
    "                'predicted_word': predicted_word,\n",
    "                'top5': words_only\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def save_log():\n",
    "    path = 'data/gensim/log/'\n",
    "    model_name = MODEL_NAME\n",
    "        \n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    print(path)\n",
    "    with open(f'{path}/log-{model_name}.txt', 'w') as f:\n",
    "        f.write(f\"*{model_name}*\\n\")\n",
    "        f.write(f\"Word analogies accuracy: {word_analogy_acc:.2%}, {df['is_correct'].astype(int).sum()}/{len(df)}\\n\")\n",
    "        f.write(f\"Analogies CSV total len: {len(csv_df)}\\n\")\n",
    "\n",
    "if GENERATE_LOG:\n",
    "    csv_df = pd.read_csv(RELATIONS_CSV)\n",
    "    df = process_relations(csv_df)\n",
    "\n",
    "    word_analogy_acc = df['is_correct'].astype(int).sum()/len(df)\n",
    "    \n",
    "    save_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/kl/kl-{SIZE}.txt', 'r') as arq:\n",
    "    kl_divergences = eval(arq.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = build_corpus(SIZE, return_fields=['corpus', 'word2idx', 'idx2word', 'word_count'],  load=True)\n",
    "corpus, w2idx, idx2w, wc = data['corpus'], data['word2idx'], data['idx2word'], data['word_count']\n",
    "filtered_words = {key: value for key, value in wc.items() if value >=10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(embedding):\n",
    "    return np.linalg.norm(embedding)\n",
    "\n",
    "data = [(norm(model.wv[word]) if word in model.wv else 0, kl_divergences[word]) for word in filtered_words.keys()]\n",
    "\n",
    "y = [data[i][1] for i in range(len(data)) if data[i][0] != 0]\n",
    "x = [data[i][0] for i in range(len(data)) if data[i][0] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['font.family'] = 'Ubuntu'\n",
    "\n",
    "grid_size = 50\n",
    "hist, x_edges, y_edges = np.histogram2d(x, y, bins=grid_size)\n",
    "\n",
    "x_idx = np.clip(np.searchsorted(x_edges, x, side='right') - 1, 0, hist.shape[0] - 1)\n",
    "y_idx = np.clip(np.searchsorted(y_edges, y, side='right') - 1, 0, hist.shape[1] - 1)\n",
    "\n",
    "density = hist[x_idx, y_idx]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "scatter = plt.scatter(x, y, c=density, cmap='viridis', alpha=0.8, s=50)\n",
    "\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Density', fontsize=12)\n",
    "\n",
    "m, b, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "ax.plot(x, m * np.array(x) + b, color='black', label='Regression Line')\n",
    "ax.legend()\n",
    "\n",
    "x_mean, y_mean = np.mean(x)*2.08, np.mean(y)*0.18\n",
    "ax.annotate(f'rÂ²: {r_value**2:.2f}', xy=(x_mean, y_mean + 1 * y_mean), fontsize=11)\n",
    "ax.annotate(f'formula: {m:.2f}x + {b:.2f}', xy=(x_mean, y_mean), fontsize=11)\n",
    "\n",
    "ax.set_title(\"Norm x KL Divergence - Regular model\", fontsize=14)\n",
    "ax.set_xlabel(\"Vector norm\", fontsize=12)\n",
    "ax.set_ylabel(\"KL Divergence\", fontsize=12)\n",
    "\n",
    "ax.set_xlim(-1, 11)\n",
    "ax.set_ylim(-1, 13)\n",
    "\n",
    "plt.savefig('plots/kl_norm_gensim_50k_8e_8w.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
