{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from src.pytorch_word2vec import neg_skipgram\n",
    "from src.dataset import build_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 50000\n",
    "RELATIONS_CSV = 'data/filtered-questions-words.csv'\n",
    "MODEL_NAME = 'pytorch_model_50k_8'\n",
    "VARIANT = 'reg1'\n",
    "VOCAB_SIZE = 161333 if SIZE == 10000 else 512461\n",
    "\n",
    "GENERATE_WORD_RELATIONS_LOG = False\n",
    "\n",
    "pytorch_model = neg_skipgram(vocab_size=VOCAB_SIZE,\n",
    "                             w2idx_path=f'data/{MODEL_NAME}/{VARIANT}/w2idx.pkl',\n",
    "                             embedding_dimension=300,\n",
    "                             regularization=0.01)\n",
    "\n",
    "pytorch_model.load_state_dict(torch.load(f'data/{MODEL_NAME}/{VARIANT}/7_w2v.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relation(row):\n",
    "    try:\n",
    "        word_one = row['word_one']\n",
    "        word_two = row['word_two']\n",
    "        word_three = row['word_three']\n",
    "        word_four = row['word_four']\n",
    "        \n",
    "        embedding_two = pytorch_model.embed(word_two)\n",
    "        embedding_one = pytorch_model.embed(word_one)\n",
    "        embedding_three = pytorch_model.embed(word_three)\n",
    "        \n",
    "        predicted_vector = embedding_two - embedding_one + embedding_three\n",
    "\n",
    "        most_similar = pytorch_model.get_similar(predicted_vector, n=5)\n",
    "        words_only = [word for word, similarity in most_similar]\n",
    "\n",
    "        return word_four in words_only, most_similar[0][0], words_only\n",
    "    except KeyError:\n",
    "        return None, None, None\n",
    "\n",
    "def process_relations(df):\n",
    "    results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        is_correct, predicted_word, words_only = check_relation(row)\n",
    "        if predicted_word is not None:\n",
    "            results.append({\n",
    "                'row_id': row['row_id'],\n",
    "                'category': row['category'],\n",
    "                'word_one': row['word_one'],\n",
    "                'word_two': row['word_two'],\n",
    "                'word_three': row['word_three'],\n",
    "                'word_four': row['word_four'],\n",
    "                'is_correct': is_correct,\n",
    "                'predicted_word': predicted_word,\n",
    "                'top5': words_only\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def save_log():\n",
    "    path = f'data/{MODEL_NAME}/{VARIANT}/log/'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    print(path)\n",
    "    with open(f'{path}/log-{VARIANT}.txt', 'w') as f:\n",
    "        f.write(f\"*{VARIANT}*\\n\")\n",
    "        f.write(f\"Word analogies accuracy: {word_analogy_acc:.2%}, {df['is_correct'].astype(int).sum()}/{len(df)}\\n\")\n",
    "        f.write(f\"Analogies CSV total len: {len(csv_df)}\\n\")\n",
    "\n",
    "if GENERATE_WORD_RELATIONS_LOG:\n",
    "    csv_df = pd.read_csv(RELATIONS_CSV)\n",
    "    df = process_relations(csv_df)\n",
    "\n",
    "    word_analogy_acc = df['is_correct'].astype(int).sum()/len(df)\n",
    "    save_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL Divergence x Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/kl/kl-{SIZE}.txt', 'r') as arq:\n",
    "    kl_divergences = eval(arq.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = build_corpus(SIZE, return_fields=['corpus', 'word2idx', 'idx2word', 'word_count'],  load=True)\n",
    "corpus, w2idx, idx2w, wc = data['corpus'], data['word2idx'], data['idx2word'], data['word_count']\n",
    "filtered_words = {key: value for key, value in wc.items() if value >=10}\n",
    "len(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(embedding):\n",
    "    return np.linalg.norm(embedding)\n",
    "\n",
    "x, y = zip(*[(norm(pytorch_model.embed(word)), kl_divergences[word]) \n",
    "             for word in filtered_words.keys() if word in pytorch_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['font.family'] = 'Ubuntu'\n",
    "\n",
    "grid_size = 50\n",
    "hist, x_edges, y_edges = np.histogram2d(x, y, bins=grid_size)\n",
    "\n",
    "x_idx = np.clip(np.searchsorted(x_edges, x, side='right') - 1, 0, hist.shape[0] - 1)\n",
    "y_idx = np.clip(np.searchsorted(y_edges, y, side='right') - 1, 0, hist.shape[1] - 1)\n",
    "\n",
    "density = hist[x_idx, y_idx]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "scatter = plt.scatter(x, y, c=density, cmap='viridis', alpha=0.8, s=50)\n",
    "\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Density', fontsize=12)\n",
    "\n",
    "m, b, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "ax.plot(x, m * np.array(x) + b, color='black', label='Regression Line')\n",
    "ax.legend()\n",
    "\n",
    "x_mean, y_mean = np.mean(x)*1.32, np.mean(y)*(np.mean(y)-2.4)/np.mean(y)\n",
    "ax.annotate(f'rÂ²: {r_value**2:.2f}', xy=(x_mean, y_mean + 1 * y_mean), fontsize=11)\n",
    "ax.annotate(f'formula: {m:.2f}x + {b:.2f}', xy=(x_mean, y_mean), fontsize=11)\n",
    "\n",
    "ax.set_title(\"Norm x KL Divergence - Reg1 model\", fontsize=14)\n",
    "ax.set_xlabel(\"Vector norm\", fontsize=12)\n",
    "ax.set_ylabel(\"KL Divergence\", fontsize=12)\n",
    "\n",
    "\n",
    "plt.savefig('plots/kl_norm_our_model_reg1_50k_8e_8w.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
