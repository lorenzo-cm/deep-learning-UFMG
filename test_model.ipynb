{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "from src.pytorch_word2vec import neg_skipgram\n",
    "\n",
    "from src.dataset import build_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 50000\n",
    "MODEL_NAME = 'model_50k_300_8_10_1'\n",
    "RELATIONS_CSV = 'data/filtered-questions-words.csv'\n",
    "\n",
    "model = Word2Vec.load(f'data/gensim/models/{MODEL_NAME}.model')\n",
    "\n",
    "pytorch_model = neg_skipgram(161333, 300, 0.01)\n",
    "pytorch_model.load_state_dict(torch.load('data/pytorch_model/default/24_w2v.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word, is_pytorch=True):\n",
    "    if is_pytorch:\n",
    "        return pytorch_model.embed(word)\n",
    "    else:\n",
    "        return model.wv[word]\n",
    "    \n",
    "def get_similar(vector, n, is_pytorch=True):\n",
    "    if is_pytorch:\n",
    "        return pytorch_model.get_similar(vector, n)\n",
    "    else:\n",
    "        return model.wv.similar_by_vector(vector, topn=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_relation(row, is_pytorch):\n",
    "    try:\n",
    "        word_one = row['word_one']\n",
    "        word_two = row['word_two']\n",
    "        word_three = row['word_three']\n",
    "        word_four = row['word_four']\n",
    "        \n",
    "        embedding_two = get_embedding(word_two, is_pytorch)\n",
    "        embedding_one = get_embedding(word_one, is_pytorch)\n",
    "        embedding_three = get_embedding(word_three, is_pytorch)\n",
    "        \n",
    "        predicted_vector = embedding_two - embedding_one + embedding_three\n",
    "\n",
    "        most_similar = get_similar(predicted_vector, n=5, is_pytorch=is_pytorch)\n",
    "        words_only = [word for word, similarity in most_similar]\n",
    "\n",
    "        return word_four in words_only, most_similar[0][0], words_only\n",
    "    except KeyError:\n",
    "        return None, None, None\n",
    "\n",
    "def process_relations(df, is_pytorch):\n",
    "    results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        is_correct, predicted_word, words_only = check_relation(row, is_pytorch)\n",
    "        if predicted_word is not None:\n",
    "            results.append({\n",
    "                'row_id': row['row_id'],\n",
    "                'category': row['category'],\n",
    "                'word_one': row['word_one'],\n",
    "                'word_two': row['word_two'],\n",
    "                'word_three': row['word_three'],\n",
    "                'word_four': row['word_four'],\n",
    "                'is_correct': is_correct,\n",
    "                'predicted_word': predicted_word,\n",
    "                'top5': words_only\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "csv_df = pd.read_csv(RELATIONS_CSV)\n",
    "df = process_relations(csv_df, True)\n",
    "\n",
    "word_analogy_acc = df['is_correct'].astype(int).sum()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_log(is_pytorch):\n",
    "    \n",
    "    if is_pytorch:\n",
    "        path = 'data/pytorch_model/default/log/'\n",
    "        model_name = 'pytorch'\n",
    "    else:\n",
    "        path = 'data/gensim/log/'\n",
    "        model_name = MODEL_NAME\n",
    "        \n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    print(path)\n",
    "    with open(f'{path}/log-{model_name}.txt', 'w') as f:\n",
    "        f.write(f\"*{model_name}*\\n\")\n",
    "        f.write(f\"Word analogies accuracy: {word_analogy_acc:.2%}, {df['is_correct'].astype(int).sum()}/{len(df)}\\n\")\n",
    "        f.write(f\"Analogies CSV total len: {len(csv_df)}\\n\")\n",
    "        \n",
    "save_log(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_analogy_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arq = open(f'data/kl/kl-{SIZE}.txt', 'r')\n",
    "content = arq.read()\n",
    "arq.close()\n",
    "\n",
    "kl_divergences = eval(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = build_corpus(SIZE, return_fields=['corpus', 'word2idx', 'idx2word', 'word_count'],  load=True)\n",
    "corpus, w2idx, idx2w, wc = data['corpus'], data['word2idx'], data['idx2word'], data['word_count']\n",
    "filtered_words = {key: value for key, value in wc.items() if value >=10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the relation- not adapted to pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(embedding):\n",
    "    return np.linalg.norm(embedding)\n",
    "\n",
    "data = [(norm(model.wv[word]) if word in model.wv else 0, kl_divergences[word]) for word in filtered_words.keys()]\n",
    "\n",
    "y = [data[i][1] for i in range(len(data)) if data[i][0] != 0]\n",
    "x = [data[i][0] for i in range(len(data)) if data[i][0] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 50\n",
    "hist, x_edges, y_edges = np.histogram2d(x, y, bins=grid_size)\n",
    "\n",
    "x_idx = np.clip(np.searchsorted(x_edges, x, side='right') - 1, 0, hist.shape[0] - 1)\n",
    "y_idx = np.clip(np.searchsorted(y_edges, y, side='right') - 1, 0, hist.shape[1] - 1)\n",
    "\n",
    "density = hist[x_idx, y_idx]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "scatter = plt.scatter(x, y, c=density, cmap='viridis', alpha=0.8, edgecolors='k', s=50)\n",
    "\n",
    "plt.title(\"Norma x Divergência KL\", fontsize=14)\n",
    "plt.xlabel(\"Norma do Vetor\", fontsize=12)\n",
    "plt.ylabel(\"Divergência KL\", fontsize=12)\n",
    "\n",
    "cbar = plt.colorbar(scatter)\n",
    "plt.xlim(0, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "scatter = plt.scatter(x, y, c=y, cmap='viridis', alpha=0.8, edgecolors='k', s=50)\n",
    "\n",
    "plt.title(\"Norma x Divergência KL\", fontsize=14)\n",
    "plt.xlabel(\"Norma do Vetor\", fontsize=12)\n",
    "plt.ylabel(\"Divergência KL\", fontsize=12)\n",
    "\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label(\"Divergência KL\", fontsize=12)\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xlim(0, 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [item[0] for item in data]  # Todos os valores de norm(model.wv[word])\n",
    "y = [item[1] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model = LinearRegression()\n",
    "regression_model.fit(np.array(X).reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coeficiente:\", regression_model.coef_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
